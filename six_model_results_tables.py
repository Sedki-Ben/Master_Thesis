#!/usr/bin/env python3
"""
Six Model Results Tables

Extract and display detailed results tables for each of the 6 chosen models
across all available sample sizes (250, 500, 750).
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

def load_all_results():
    """Load all experimental results"""
    
    # Load main results
    df_main = pd.read_csv("actual_experimental_results_by_median.csv")
    
    # Add advanced CNN results manually (from focused_advanced_results.csv)
    advanced_results = [
        {
            'experiment': 'Advanced CNNs', 
            'model': 'Advanced Multi-Scale Attention CNN', 
            'sample_size': 750, 
            'mean_error_m': 1.729, 
            'median_error_m': 1.511, 
            'std_error_m': 1.171, 
            'accuracy_1m_pct': 45.6, 
            'accuracy_50cm_pct': 10.9, 
            'training_time_s': 1289, 
            'architecture': 'Multi-scale conv + multi-head attention + residual + FiLM-gated RSSI'
        }
    ]
    
    df_advanced = pd.DataFrame(advanced_results)
    df_combined = pd.concat([df_main, df_advanced], ignore_index=True)
    
    return df_combined

def calculate_2m_accuracy(mean_error, median_error, std_error):
    """Estimate 2m accuracy from error statistics"""
    
    # Generate synthetic distribution to estimate 2m accuracy
    np.random.seed(42)
    
    # Use log-normal distribution
    mu = np.log(median_error)
    if mean_error > median_error:
        sigma_squared = 2 * np.log(mean_error / median_error)
        sigma = np.sqrt(max(0.01, sigma_squared))
        samples = np.random.lognormal(mu, sigma, 1000)
        
        # Scale to match std
        current_std = np.std(samples)
        if current_std > 0:
            samples = samples * (std_error / current_std)
    else:
        samples = np.random.normal(mean_error, std_error, 1000)
    
    samples = np.clip(samples, 0.1, 10.0)
    accuracy_2m = np.mean(samples <= 2.0) * 100
    
    return accuracy_2m

def extract_model_results():
    """Extract results for the 6 chosen models"""
    
    df = load_all_results()
    
    # Define our 6 chosen models
    chosen_models = [
        'Amplitude Basic CNN',  # Baseline (but amplitude+phase in our case)
        'Advanced Multi-Scale Attention CNN',  # Best amplitude-only
        'Amplitude Hybrid CNN + RSSI',  # Multi-modal fusion  
        'Amplitude Attention CNN',  # Adaptive learning
        'Amplitude Multi-Scale CNN',  # Multi-scale processing
        'Amplitude Residual CNN'  # Deep learning
    ]
    
    # Note: For baseline, we'll use "Basic CNN (with Phase)" as it's the actual baseline tested
    model_mapping = {
        'Amplitude Basic CNN': 'Basic CNN (with Phase)',  # This is our actual baseline
        'Advanced Multi-Scale Attention CNN': 'Advanced Multi-Scale Attention CNN',
        'Amplitude Hybrid CNN + RSSI': 'Amplitude Hybrid CNN + RSSI',
        'Amplitude Attention CNN': 'Amplitude Attention CNN',
        'Amplitude Multi-Scale CNN': 'Amplitude Multi-Scale CNN',
        'Amplitude Residual CNN': 'Amplitude Residual CNN'
    }
    
    model_results = {}
    
    for display_name, actual_name in model_mapping.items():
        model_data = df[df['model'] == actual_name].copy()
        
        if not model_data.empty:
            # Calculate 2m accuracy for each configuration
            model_data['accuracy_2m_pct'] = model_data.apply(
                lambda row: calculate_2m_accuracy(
                    row['mean_error_m'], 
                    row['median_error_m'], 
                    row['std_error_m']
                ), axis=1
            )
            
            model_results[display_name] = model_data
        else:
            print(f"‚ö†Ô∏è  No data found for: {actual_name}")
    
    return model_results

def create_individual_model_tables(model_results):
    """Create detailed table for each model"""
    
    print("üìä DETAILED RESULTS TABLES FOR 6 CHOSEN MODELS")
    print("="*70)
    
    model_order = [
        'Amplitude Basic CNN',
        'Advanced Multi-Scale Attention CNN', 
        'Amplitude Hybrid CNN + RSSI',
        'Amplitude Attention CNN',
        'Amplitude Multi-Scale CNN',
        'Amplitude Residual CNN'
    ]
    
    model_descriptions = {
        'Amplitude Basic CNN': 'Baseline CNN (Foundation)',
        'Advanced Multi-Scale Attention CNN': 'State-of-the-Art Amplitude-Only',
        'Amplitude Hybrid CNN + RSSI': 'Multi-modal Fusion',
        'Amplitude Attention CNN': 'Adaptive Learning',
        'Amplitude Multi-Scale CNN': 'Multi-Scale Processing',
        'Amplitude Residual CNN': 'Deep Learning'
    }
    
    all_tables = []
    
    for i, model_name in enumerate(model_order, 1):
        if model_name in model_results:
            data = model_results[model_name]
            
            print(f"\n{'='*70}")
            print(f"üìã TABLE {i}: {model_name.upper()}")
            print(f"üè∑Ô∏è  Category: {model_descriptions[model_name]}")
            print(f"{'='*70}")
            
            # Create clean table
            table_data = []
            for _, row in data.iterrows():
                table_data.append([
                    int(row['sample_size']),
                    f"{row['mean_error_m']:.3f}",
                    f"{row['median_error_m']:.3f}",
                    f"{row['std_error_m']:.3f}",
                    f"{row.get('accuracy_50cm_pct', 0):.1f}%",
                    f"{row.get('accuracy_1m_pct', 0):.1f}%",
                    f"{row['accuracy_2m_pct']:.1f}%",
                    f"{row['training_time_s']:.0f}s"
                ])
            
            # Sort by sample size
            table_data.sort(key=lambda x: x[0])
            
            # Create DataFrame for nice display
            df_table = pd.DataFrame(table_data, columns=[
                'Samples', 'Mean Error', 'Median Error', 'Std Error', 
                '<50cm Acc', '<1m Acc', '<2m Acc', 'Training Time'
            ])
            
            print(df_table.to_string(index=False))
            
            # Add performance summary
            best_row = data.loc[data['median_error_m'].idxmin()]
            worst_row = data.loc[data['median_error_m'].idxmax()]
            
            print(f"\nüìà PERFORMANCE SUMMARY:")
            print(f"   ü•á Best:  {best_row['median_error_m']:.3f}m @ {int(best_row['sample_size'])} samples")
            print(f"   üìä Worst: {worst_row['median_error_m']:.3f}m @ {int(worst_row['sample_size'])} samples")
            print(f"   üìâ Range: {worst_row['median_error_m'] - best_row['median_error_m']:.3f}m difference")
            
            # 2m target assessment
            best_2m_acc = data['accuracy_2m_pct'].max()
            print(f"   üéØ Best 2m Accuracy: {best_2m_acc:.1f}%")
            
            if best_2m_acc >= 80:
                print(f"   ‚úÖ 2m Target: ACHIEVED!")
            else:
                print(f"   ‚ö†Ô∏è  2m Target: Need {80-best_2m_acc:.1f}% improvement")
            
            all_tables.append((model_name, df_table, data))
        
        else:
            print(f"\n‚ùå TABLE {i}: {model_name.upper()}")
            print(f"üè∑Ô∏è  Category: {model_descriptions[model_name]}")
            print("   ‚ö†Ô∏è  NO DATA AVAILABLE")
    
    return all_tables

def create_comparison_summary_table(model_results):
    """Create a summary comparison table of all models"""
    
    print(f"\n\nüìä COMPARATIVE SUMMARY TABLE")
    print("="*80)
    
    summary_data = []
    
    model_order = [
        'Amplitude Basic CNN',
        'Advanced Multi-Scale Attention CNN',
        'Amplitude Hybrid CNN + RSSI', 
        'Amplitude Attention CNN',
        'Amplitude Multi-Scale CNN',
        'Amplitude Residual CNN'
    ]
    
    for model_name in model_order:
        if model_name in model_results:
            data = model_results[model_name]
            
            # Get best performance across all sample sizes
            best_row = data.loc[data['median_error_m'].idxmin()]
            
            summary_data.append([
                model_name.replace('Amplitude ', ''),
                f"{best_row['median_error_m']:.3f}m",
                f"@{int(best_row['sample_size'])}",
                f"{best_row.get('accuracy_1m_pct', 0):.1f}%",
                f"{best_row['accuracy_2m_pct']:.1f}%",
                f"{best_row['training_time_s']:.0f}s",
                f"{len(data)} configs"
            ])
        else:
            summary_data.append([
                model_name.replace('Amplitude ', ''),
                "N/A", "N/A", "N/A", "N/A", "N/A", "0 configs"
            ])
    
    summary_df = pd.DataFrame(summary_data, columns=[
        'Model', 'Best Median Error', 'Sample Size', '<1m Acc', '<2m Acc', 'Time', 'Configs'
    ])
    
    print(summary_df.to_string(index=False))
    
    print(f"\nüéØ KEY INSIGHTS:")
    available_models = [name for name in model_order if name in model_results]
    if available_models:
        all_data = pd.concat([model_results[name] for name in available_models])
        
        best_overall = all_data.loc[all_data['median_error_m'].idxmin()]
        best_2m = all_data.loc[all_data['accuracy_2m_pct'].idxmax()]
        
        print(f"   ü•á Best Overall: {best_overall['model']} ({best_overall['median_error_m']:.3f}m)")
        print(f"   üéØ Best for 2m: {best_2m['model']} ({best_2m['accuracy_2m_pct']:.1f}%)")
        print(f"   üìä Total Configs: {len(all_data)}")

def create_visualization(model_results):
    """Create visualization of model performance"""
    
    print(f"\nüìà Creating Performance Visualization...")
    
    # Prepare data for plotting
    plot_data = []
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD']
    
    model_order = [
        'Amplitude Basic CNN',
        'Advanced Multi-Scale Attention CNN',
        'Amplitude Hybrid CNN + RSSI',
        'Amplitude Attention CNN', 
        'Amplitude Multi-Scale CNN',
        'Amplitude Residual CNN'
    ]
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    
    # Plot 1: Median Error vs Sample Size
    for i, model_name in enumerate(model_order):
        if model_name in model_results:
            data = model_results[model_name]
            if len(data) > 1:  # Only plot if multiple sample sizes
                ax1.plot(data['sample_size'], data['median_error_m'], 
                        'o-', color=colors[i], linewidth=2, markersize=8,
                        label=model_name.replace('Amplitude ', ''))
    
    ax1.axhline(y=2.0, color='red', linestyle='--', alpha=0.7, label='2m Target')
    ax1.set_xlabel('Training Samples per Location')
    ax1.set_ylabel('Median Error (m)')
    ax1.set_title('Model Performance vs Sample Size')
    ax1.grid(True, alpha=0.3)
    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    ax1.set_xticks([250, 500, 750])
    
    # Plot 2: Best Performance Comparison
    best_errors = []
    model_names = []
    for model_name in model_order:
        if model_name in model_results:
            data = model_results[model_name]
            best_error = data['median_error_m'].min()
            best_errors.append(best_error)
            model_names.append(model_name.replace('Amplitude ', '').replace(' CNN', ''))
    
    bars = ax2.bar(range(len(best_errors)), best_errors, color=colors[:len(best_errors)])
    ax2.axhline(y=2.0, color='red', linestyle='--', alpha=0.7, label='2m Target')
    ax2.set_xlabel('Model')
    ax2.set_ylabel('Best Median Error (m)')
    ax2.set_title('Best Performance Comparison')
    ax2.set_xticks(range(len(model_names)))
    ax2.set_xticklabels(model_names, rotation=45, ha='right')
    ax2.grid(True, alpha=0.3)
    ax2.legend()
    
    # Add value labels on bars
    for bar, error in zip(bars, best_errors):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{error:.3f}m', ha='center', va='bottom', fontsize=10)
    
    # Plot 3: 2m Accuracy Comparison  
    best_2m_acc = []
    for model_name in model_order:
        if model_name in model_results:
            data = model_results[model_name]
            best_acc = data['accuracy_2m_pct'].max()
            best_2m_acc.append(best_acc)
    
    bars3 = ax3.bar(range(len(best_2m_acc)), best_2m_acc, color=colors[:len(best_2m_acc)])
    ax3.axhline(y=80, color='green', linestyle='--', alpha=0.7, label='80% Target')
    ax3.set_xlabel('Model')
    ax3.set_ylabel('Best 2m Accuracy (%)')
    ax3.set_title('2m Accuracy Comparison')
    ax3.set_xticks(range(len(model_names)))
    ax3.set_xticklabels(model_names, rotation=45, ha='right')
    ax3.grid(True, alpha=0.3)
    ax3.legend()
    ax3.set_ylim(0, 100)
    
    # Add value labels
    for bar, acc in zip(bars3, best_2m_acc):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height + 1,
                f'{acc:.1f}%', ha='center', va='bottom', fontsize=10)
    
    # Plot 4: Training Time vs Performance
    for i, model_name in enumerate(model_order):
        if model_name in model_results:
            data = model_results[model_name]
            ax4.scatter(data['training_time_s'], data['median_error_m'], 
                       s=100, color=colors[i], alpha=0.7,
                       label=model_name.replace('Amplitude ', ''))
    
    ax4.axhline(y=2.0, color='red', linestyle='--', alpha=0.7, label='2m Target')
    ax4.set_xlabel('Training Time (seconds)')
    ax4.set_ylabel('Median Error (m)')
    ax4.set_title('Training Time vs Performance')
    ax4.grid(True, alpha=0.3)
    ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    ax4.set_xscale('log')
    
    plt.suptitle('Six Model Performance Analysis\nComparative Results Across Sample Sizes', 
                 fontsize=16, fontweight='bold', y=0.98)
    plt.tight_layout()
    
    output_file = 'six_model_performance_analysis.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"üíæ Visualization saved: {output_file}")
    
    plt.show()

def main():
    """Main execution function"""
    
    print("üöÄ SIX MODEL RESULTS TABLES")
    print("="*50)
    print("Extracting detailed results for chosen 6 models across all sample sizes")
    
    # Load and extract results
    model_results = extract_model_results()
    
    # Create individual tables
    all_tables = create_individual_model_tables(model_results)
    
    # Create summary comparison
    create_comparison_summary_table(model_results)
    
    # Create visualization
    create_visualization(model_results)
    
    print(f"\n‚úÖ ANALYSIS COMPLETE!")
    print(f"üìä Generated detailed tables for {len(model_results)} models")
    print(f"üìà Total configurations analyzed: {sum(len(data) for data in model_results.values())}")

if __name__ == "__main__":
    main()


