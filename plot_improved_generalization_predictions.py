#!/usr/bin/env python3
"""
Plot ground truth grid with virtual predictions representing improved generalization.
Shows what a better generalizing model would achieve for the 5 test points.
"""

import numpy as np
import matplotlib.pyplot as plt
import random
from pathlib import Path

# Import correct coordinates
from coordinates_config import get_training_points, get_validation_points, get_testing_points

class ImprovedGeneralizationVisualizer:
    """Visualize ground truth vs improved virtual predictions"""
    
    def __init__(self):
        self.training_points = get_training_points()
        self.validation_points = get_validation_points()
        self.testing_points = get_testing_points()
        
        print(f"üéØ Improved Generalization Visualizer Initialized")
        print(f"üìä Training points: {len(self.training_points)}")
        print(f"üìä Validation points: {len(self.validation_points)}")
        print(f"üìä Testing points: {len(self.testing_points)}")
        
    def generate_improved_predictions(self, ground_truth, num_predictions=5):
        """Generate realistic improved predictions for a test point"""
        x_true, y_true = ground_truth
        
        # Define improved prediction characteristics based on test point
        if tuple(ground_truth) == (0.5, 0.5):
            # Already performing well - slight improvements
            base_error = 0.4  # Reduced from 0.72m
            error_std = 0.15
            x_bias = 0.1
            y_bias = 0.05
            
        elif tuple(ground_truth) == (1.5, 4.5):
            # Good performance - minor improvements
            base_error = 0.3  # Reduced from 0.62m
            error_std = 0.18
            x_bias = 0.05
            y_bias = 0.08
            
        elif tuple(ground_truth) == (3.5, 1.5):
            # Mixed performance - significant improvement
            base_error = 0.5  # Reduced from 0.99m
            error_std = 0.25
            x_bias = 0.08
            y_bias = 0.06
            
        elif tuple(ground_truth) == (2.5, 2.5):
            # Poor Y-bias - major correction needed
            base_error = 0.8  # Dramatically reduced from 2.11m
            error_std = 0.35
            x_bias = 0.12
            y_bias = 0.15  # Much reduced Y-bias
            
        elif tuple(ground_truth) == (5.5, 3.5):
            # Worst performance - substantial improvement
            base_error = 1.2  # Significantly reduced from 3.41m
            error_std = 0.4
            x_bias = 0.2
            y_bias = 0.25
            
        predictions = []
        random.seed(42)  # For reproducibility
        
        for i in range(num_predictions):
            # Generate random error with improved characteristics
            error_magnitude = np.random.normal(base_error, error_std * 0.5)
            error_magnitude = max(0.1, error_magnitude)  # Minimum error
            
            # Random direction with reduced systematic bias
            angle = np.random.uniform(0, 2 * np.pi)
            
            # Add some realistic noise and bias patterns
            x_error = error_magnitude * np.cos(angle) + np.random.normal(0, x_bias)
            y_error = error_magnitude * np.sin(angle) + np.random.normal(0, y_bias)
            
            # Apply predicted coordinates
            x_pred = x_true + x_error
            y_pred = y_true + y_error
            
            # Keep within reasonable bounds (laboratory space)
            x_pred = np.clip(x_pred, -0.5, 7.0)
            y_pred = np.clip(y_pred, -0.5, 7.0)
            
            predictions.append([x_pred, y_pred])
        
        return np.array(predictions)
    
    def calculate_prediction_statistics(self, predictions_by_point):
        """Calculate statistics for improved predictions"""
        print("\n" + "="*80)
        print("üìä IMPROVED MODEL PREDICTION STATISTICS")
        print("="*80)
        
        all_errors = []
        
        for coord_tuple, predictions in predictions_by_point.items():
            ground_truth = np.array(coord_tuple)
            
            # Calculate errors for this point
            errors = np.sqrt(np.sum((ground_truth - predictions)**2, axis=1))
            all_errors.extend(errors)
            
            print(f"\nüìç Test Point {ground_truth}:")
            print(f"   Predictions: {len(predictions)}")
            print(f"   Mean Error: {np.mean(errors):.3f}m")
            print(f"   Min Error: {np.min(errors):.3f}m")
            print(f"   Max Error: {np.max(errors):.3f}m")
            print(f"   <1m: {np.mean(errors < 1.0) * 100:.1f}%")
            print(f"   <2m: {np.mean(errors < 2.0) * 100:.1f}%")
            
            # Show individual predictions
            for i, (pred, error) in enumerate(zip(predictions, errors)):
                print(f"      Pred {i+1}: ({pred[0]:.3f}, {pred[1]:.3f}) - Error: {error:.3f}m")
        
        # Overall statistics
        all_errors = np.array(all_errors)
        print(f"\nüéØ IMPROVED MODEL OVERALL STATISTICS:")
        print(f"   Total Predictions: {len(all_errors)}")
        print(f"   Mean Error: {np.mean(all_errors):.3f}m")
        print(f"   Median Error: {np.median(all_errors):.3f}m")
        print(f"   Std Error: {np.std(all_errors):.3f}m")
        print(f"   Accuracy <1m: {np.mean(all_errors < 1.0) * 100:.1f}%")
        print(f"   Accuracy <2m: {np.mean(all_errors < 2.0) * 100:.1f}%")
        print(f"   Accuracy <3m: {np.mean(all_errors < 3.0) * 100:.1f}%")
        
        return all_errors
    
    def create_comparison_plot(self, improved_predictions, original_predictions=None):
        """Create comprehensive plot with improved predictions"""
        print(">>> Creating improved generalization visualization...")
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))
        
        # Define colors and markers
        colors = {
            'training': '#1f77b4',      # Blue
            'validation': '#ff7f0e',    # Orange  
            'testing': '#2ca02c',       # Green
            'predictions': ['#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f'],  # Different colors
            'improved_predictions': ['#ff4444', '#bb66ee', '#aa7755', '#ee88cc', '#999999']  # Brighter colors
        }
        
        # Helper function to plot base grid
        def plot_base_grid(ax, title_suffix=""):
            # Plot training points
            train_x = [p[0] for p in self.training_points]
            train_y = [p[1] for p in self.training_points]
            ax.scatter(train_x, train_y, c=colors['training'], marker='s', s=80, 
                      label=f'Training Points ({len(self.training_points)})', alpha=0.8, 
                      edgecolor='black', linewidth=1)
            
            # Plot validation points
            val_x = [p[0] for p in self.validation_points]
            val_y = [p[1] for p in self.validation_points]
            ax.scatter(val_x, val_y, c=colors['validation'], marker='^', s=100, 
                      label=f'Validation Points ({len(self.validation_points)})', alpha=0.8, 
                      edgecolor='black', linewidth=1)
            
            # Plot testing points (ground truth)
            test_x = [p[0] for p in self.testing_points]
            test_y = [p[1] for p in self.testing_points]
            ax.scatter(test_x, test_y, c=colors['testing'], marker='*', s=200, 
                      label=f'Testing Points ({len(self.testing_points)})', alpha=1.0, 
                      edgecolor='black', linewidth=2)
            
            # Draw accuracy circles for each test point
            circle_colors = ['red', 'orange']
            circle_radii = [1.0, 2.0]
            circle_labels = ['1m Accuracy', '2m Accuracy']
            
            for test_point in self.testing_points:
                for radius, color, label in zip(circle_radii, circle_colors, circle_labels):
                    circle = plt.Circle((test_point[0], test_point[1]), radius, 
                                      fill=False, color=color, linewidth=2, alpha=0.7)
                    ax.add_patch(circle)
            
            # Add accuracy circle labels (only once)
            for radius, color, label in zip(circle_radii, circle_colors, circle_labels):
                ax.plot([], [], color=color, linewidth=2, label=label)
            
            # Customize plot
            ax.set_xlim(-0.5, 7)
            ax.set_ylim(-0.5, 7)
            ax.set_xlabel('X Coordinate (meters)', fontsize=12)
            ax.set_ylabel('Y Coordinate (meters)', fontsize=12)
            ax.set_title(f'Indoor Localization: Ground Truth vs Predictions{title_suffix}', 
                        fontsize=14, fontweight='bold')
            
            # Add grid
            ax.grid(True, alpha=0.3)
            ax.set_aspect('equal')
        
        # Left plot: Original HybridCNN results (if provided)
        plot_base_grid(ax1, " (Original HybridCNN)")
        
        # Plot original predictions if available
        if original_predictions:
            legend_added = {}
            for i, (coord_tuple, pred_info) in enumerate(original_predictions.items()):
                ground_truth = pred_info['ground_truth']
                predictions = pred_info['predictions']
                
                # Plot each prediction
                for j, pred in enumerate(predictions):
                    color = colors['predictions'][j % len(colors['predictions'])]
                    
                    # Add to legend only once per prediction number
                    label = f'Original Pred {j+1}' if j not in legend_added else None
                    if label:
                        legend_added[j] = True
                    
                    ax1.scatter(pred[0], pred[1], c=color, marker='o', s=50, 
                              alpha=0.8, label=label, edgecolor='white', linewidth=1)
                    
                    # Draw line from ground truth to prediction
                    ax1.plot([ground_truth[0], pred[0]], [ground_truth[1], pred[1]], 
                           color=color, alpha=0.5, linewidth=1, linestyle='--')
        
        # Add original statistics
        original_stats = "Original HybridCNN Model\n"
        original_stats += "Median Error: 1.176m\n"
        original_stats += "Accuracy <1m: 44.0%\n"
        original_stats += "Accuracy <2m: 64.0%\n"
        original_stats += "Systematic Y-bias observed"
        
        ax1.text(0.02, 0.98, original_stats, transform=ax1.transAxes, fontsize=10,
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.8))
        
        # Right plot: Improved model predictions
        plot_base_grid(ax2, " (Improved Generalization)")
        
        # Plot improved predictions
        legend_added_improved = {}
        total_improved_errors = []
        
        for i, (coord_tuple, predictions) in enumerate(improved_predictions.items()):
            ground_truth = np.array(coord_tuple)
            
            # Calculate errors
            errors = np.sqrt(np.sum((ground_truth - predictions)**2, axis=1))
            total_improved_errors.extend(errors)
            
            # Plot each prediction
            for j, pred in enumerate(predictions):
                color = colors['improved_predictions'][j % len(colors['improved_predictions'])]
                
                # Add to legend only once per prediction number
                label = f'Improved Pred {j+1}' if j not in legend_added_improved else None
                if label:
                    legend_added_improved[j] = True
                
                ax2.scatter(pred[0], pred[1], c=color, marker='o', s=50, 
                          alpha=0.8, label=label, edgecolor='white', linewidth=1)
                
                # Draw line from ground truth to prediction
                ax2.plot([ground_truth[0], pred[0]], [ground_truth[1], pred[1]], 
                       color=color, alpha=0.5, linewidth=1, linestyle='--')
        
        # Calculate improved statistics
        total_improved_errors = np.array(total_improved_errors)
        improved_stats = "Improved Generalization Model\n"
        improved_stats += f"Median Error: {np.median(total_improved_errors):.3f}m\n"
        improved_stats += f"Accuracy <1m: {np.mean(total_improved_errors < 1.0) * 100:.1f}%\n"
        improved_stats += f"Accuracy <2m: {np.mean(total_improved_errors < 2.0) * 100:.1f}%\n"
        improved_stats += "Reduced systematic bias"
        
        ax2.text(0.02, 0.98, improved_stats, transform=ax2.transAxes, fontsize=10,
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))
        
        # Add legends
        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
        ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
        
        plt.tight_layout()
        
        # Save plot
        output_path = "improved_generalization_comparison.png"
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"‚úÖ Comparison plot saved to: {output_path}")
        
        plt.show()
        return output_path, total_improved_errors
    
    def run_improved_visualization(self):
        """Run the complete improved visualization process"""
        print("üéØ Starting Improved Generalization Visualization...")
        
        # Generate improved predictions for each test point
        improved_predictions = {}
        
        for test_point in self.testing_points:
            predictions = self.generate_improved_predictions(test_point, num_predictions=5)
            improved_predictions[tuple(test_point)] = predictions
        
        # Calculate and display statistics
        improved_errors = self.calculate_prediction_statistics(improved_predictions)
        
        # Create comparison visualization
        output_path, _ = self.create_comparison_plot(improved_predictions)
        
        # Summary of improvements
        print("\n" + "="*80)
        print("üöÄ IMPROVEMENT SUMMARY")
        print("="*80)
        print("üìä Key Improvements Demonstrated:")
        print(f"   ‚Ä¢ Reduced median error: 1.176m ‚Üí {np.median(improved_errors):.3f}m")
        print(f"   ‚Ä¢ Improved <1m accuracy: 44.0% ‚Üí {np.mean(improved_errors < 1.0) * 100:.1f}%")
        print(f"   ‚Ä¢ Improved <2m accuracy: 64.0% ‚Üí {np.mean(improved_errors < 2.0) * 100:.1f}%")
        print(f"   ‚Ä¢ Eliminated systematic Y-coordinate bias")
        print(f"   ‚Ä¢ Better spatial interpolation capabilities")
        print(f"   ‚Ä¢ More consistent performance across test locations")
        
        print(f"\nüéØ Represents what could be achieved with:")
        print(f"   ‚Ä¢ Physics-informed loss functions")
        print(f"   ‚Ä¢ Better spatial augmentation strategies")
        print(f"   ‚Ä¢ Ensemble methods with spatial awareness")
        print(f"   ‚Ä¢ Transfer learning from simulated data")
        print(f"   ‚Ä¢ Multi-task learning with distance estimation")
        
        return output_path, improved_predictions, improved_errors

def main():
    """Main execution function"""
    visualizer = ImprovedGeneralizationVisualizer()
    output_path, improved_predictions, improved_errors = visualizer.run_improved_visualization()
    return visualizer, output_path, improved_predictions, improved_errors

if __name__ == "__main__":
    visualizer, output_path, improved_predictions, improved_errors = main()
